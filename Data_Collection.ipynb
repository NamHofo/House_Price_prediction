{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "hXjA_MOs3cfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the website thoroughly to gather relevant information on each data item for house price prediction.\n",
        "\n",
        "\n",
        "1.   Website gồm nhiều trang, mỗi trang có 20 bài đăng bán nhà\n",
        "2.   Nhóm duyệt qua từng trang, ở mỗi trang tiếp tục duyệt qua từng bài đăng bán nhà\n",
        "3.   Nhóm lấy các trường dữ liệu từ trang web như sau\n",
        "    * Title\n",
        "    * Gmail\n",
        "    * Area\n",
        "    * Address\n",
        "    * Seller Name\n",
        "    * Number of bedrooms\n",
        "    * Number of bathrooms\n",
        "    * Price\n",
        "    * Content\n",
        "    * district\n"
      ],
      "metadata": {
        "id": "z0IXJmhL4YVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the size of the dataset to be collected.\n",
        "* Nhóm em cào 450 trang dữ liệu từ trang web nhưng chỉ lấy ở TP HCM. Vì dữ liệu trên web chủ yếu là thành phố HCM. Chúng em cào 450 trang là lấy được 9000 dòng dữ liệu. Các bạn dựa theo code ở dưới để chia ra cào"
      ],
      "metadata": {
        "id": "GBzLu5va5vIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4aaIJjGd3jJ"
      },
      "outputs": [],
      "source": [
        "import pandas as  pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import requests\n",
        "import re\n",
        "def download_html(url):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        html = response.read()\n",
        "        html = html.decode('utf-8')\n",
        "    response.close()\n",
        "    return html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reaC8UnxeF5N"
      },
      "outputs": [],
      "source": [
        "def scrape_data(link):\n",
        "    url=link\n",
        "    html = download_html(url)\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    house={}\n",
        "    #Lấy title\n",
        "    try:\n",
        "      title = soup.find('h1',class_='uk-panel-title').get_text()\n",
        "      title = re.sub('\\s+', ' ', title).strip()\n",
        "      house['Title'] = title\n",
        "    except:\n",
        "      house['Title']=None\n",
        "    try:\n",
        "      # lấy tên người bán\n",
        "      house['Seller Name']=soup.find('div',class_='name').find('a').get_text()\n",
        "    except:\n",
        "      house['Seller Name']=None\n",
        "    try:\n",
        "      # lấy  email người bán\n",
        "      house['Seller Email']=soup.find('div',class_='more email').find('a').get('href').replace('mailto:','')\n",
        "    except:\n",
        "      house['Seller Email']=None\n",
        "    try:\n",
        "      # lấy giá nhà\n",
        "      Gia = soup.find('strong',class_='price').get_text().strip()\n",
        "      if(Gia == 'Thỏa thuận'):\n",
        "        house['Area']=None\n",
        "        house['Address']=None\n",
        "        house['Title'] = None\n",
        "        house['Seller Name']=None\n",
        "        house['Seller Email']=None\n",
        "        house['Number of bedrooms']=None\n",
        "        house['Number of bathrooms']=None\n",
        "        house['Price']=None\n",
        "        house['Content']=None\n",
        "        house['district']=None\n",
        "\n",
        "        return house\n",
        "      else:\n",
        "        house['Price'] = Gia\n",
        "\n",
        "    except:\n",
        "      house['Price']=None\n",
        "\n",
        "    try:\n",
        "      # Lay ra quận, huyện\n",
        "      house['district'] = soup.find('ul', class_='uk-breadcrumb').find_all('li')[-1].get_text()\n",
        "    except:\n",
        "      house['district'] = None\n",
        "\n",
        "    try:\n",
        "      # vào thẻ div class param\n",
        "      param=soup.find_all('div',class_='param')[0]\n",
        "    except:\n",
        "\n",
        "      house['Area']=None\n",
        "      house['Address']=None\n",
        "      house['Number of bedrooms']=None\n",
        "      house['Number of bathrooms']=None\n",
        "      print('Here need checking')\n",
        "\n",
        "    try :\n",
        "      # lấy ra diện tích , address, số phòng ngủ và số phòng tắm từ tab param\n",
        "      house['Area']=float(re.compile(r'<strong>Diện tích:</strong>\\s*(.*?)\\s*m<sup>2</sup>').search(str(param)).group(1))\n",
        "    except:\n",
        "      house['Area']=None\n",
        "    try:\n",
        "      house['Address']=re.compile(r'<strong>Địa chỉ:</strong>\\s*(.*?)</li>').search(str(param)).group(1)\n",
        "    except:\n",
        "      house['Address']=None\n",
        "    try:\n",
        "      house['Number of bedrooms']=int(re.compile(r'<strong>Phòng ngủ:</strong>\\s*(.*?)\\s*PN').search(str(param)).group(1))\n",
        "    except:\n",
        "       house['Number of bedrooms']=None\n",
        "    try:\n",
        "      house['Number of bathrooms']=int(re.compile(r'<strong>Phòng WC:</strong>\\s*(.*?)\\s*WC').search(str(param)).group(1))\n",
        "    except:\n",
        "      house['Number of bathrooms']=None\n",
        "      #vào thẻ div content\n",
        "    try:\n",
        "      content = soup.find('div', class_='content')\n",
        "\n",
        "      # Loại bỏ tất cả các thẻ <br> và <p> trong phần tử đã tìm thấy\n",
        "      for br in content.find_all(\"br\"):\n",
        "          br.replace_with(\"\\n\")\n",
        "      for p in content.find_all(\"p\"):\n",
        "          p.unwrap()\n",
        "\n",
        "      # Chuyển đổi nội dung đã xử lý thành chuỗi\n",
        "      content_str = content.get_text()\n",
        "\n",
        "      content_str = re.sub(r'[^\\w\\s,.!?]', '', content_str)\n",
        "\n",
        "      # Loại bỏ các khoảng trắng thừa\n",
        "      content_str = re.sub('\\s+', ' ', content_str).strip()\n",
        "      house['Content'] = content_str\n",
        "    except:\n",
        "      house['Content'] = None\n",
        "\n",
        "    return house\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehi04zABeJf7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def getItems(index,maxIndex,data):#22280069\n",
        "  url='https://batdongsan.vn/ban-nha-ho-chi-minh/p1'\n",
        "  url=url.replace('/p1','/p'+str(index))\n",
        "\n",
        "  while index<maxIndex+1:\n",
        "    try:\n",
        "      print(f'-----------Crawling page {index}-----------')\n",
        "      start_time = time.time()\n",
        "      html=download_html(url)\n",
        "      soup = BeautifulSoup(html, 'lxml')\n",
        "      dataTab=soup.find('div',class_='datalist')\n",
        "      itemLinks = [link.find('a').get('href') for link in dataTab.findAll('div',class_='name')]\n",
        "      data_tempt=[scrape_data(link) for link in itemLinks]\n",
        "      for i in data_tempt:\n",
        "        data.append(i)\n",
        "      # data.append(data_tempt)\n",
        "    except Exception as e:\n",
        "      print(f\"An error occurred when crawling at page{index}:\", e)\n",
        "    url = url.replace('/p'+str(index),'/p'+str(index+1))\n",
        "    end_time = time.time()\n",
        "    print(f'Crawl page {index} successfully , time completed : {-start_time+end_time} s')\n",
        "    index+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EVVo3cYevmz",
        "outputId": "e8bd61f1-7ba4-4e3b-ad0c-06d040342a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------Crawling page 151-----------\n",
            "Crawl page 151 successfully , time completed : 72.79038214683533 s\n",
            "-----------Crawling page 152-----------\n",
            "Crawl page 152 successfully , time completed : 72.05754208564758 s\n",
            "-----------Crawling page 153-----------\n",
            "Crawl page 153 successfully , time completed : 69.74116444587708 s\n",
            "-----------Crawling page 154-----------\n",
            "Crawl page 154 successfully , time completed : 72.22955751419067 s\n",
            "-----------Crawling page 155-----------\n",
            "Crawl page 155 successfully , time completed : 74.0295820236206 s\n",
            "-----------Crawling page 156-----------\n",
            "Crawl page 156 successfully , time completed : 74.7363748550415 s\n",
            "-----------Crawling page 157-----------\n",
            "Crawl page 157 successfully , time completed : 73.77369117736816 s\n",
            "-----------Crawling page 158-----------\n",
            "Crawl page 158 successfully , time completed : 72.56320714950562 s\n",
            "-----------Crawling page 159-----------\n",
            "Crawl page 159 successfully , time completed : 72.34133696556091 s\n",
            "-----------Crawling page 160-----------\n",
            "Crawl page 160 successfully , time completed : 76.50274729728699 s\n",
            "-----------Crawling page 161-----------\n",
            "Crawl page 161 successfully , time completed : 71.13307762145996 s\n",
            "-----------Crawling page 162-----------\n",
            "Crawl page 162 successfully , time completed : 71.48827719688416 s\n",
            "-----------Crawling page 163-----------\n",
            "Crawl page 163 successfully , time completed : 72.32309746742249 s\n",
            "-----------Crawling page 164-----------\n",
            "Crawl page 164 successfully , time completed : 71.34114360809326 s\n",
            "-----------Crawling page 165-----------\n",
            "Crawl page 165 successfully , time completed : 70.63767671585083 s\n",
            "-----------Crawling page 166-----------\n",
            "Crawl page 166 successfully , time completed : 75.2381203174591 s\n",
            "-----------Crawling page 167-----------\n",
            "Crawl page 167 successfully , time completed : 71.66327929496765 s\n",
            "-----------Crawling page 168-----------\n",
            "Crawl page 168 successfully , time completed : 73.53690242767334 s\n",
            "-----------Crawling page 169-----------\n",
            "Crawl page 169 successfully , time completed : 74.46136474609375 s\n",
            "-----------Crawling page 170-----------\n",
            "Crawl page 170 successfully , time completed : 75.65036225318909 s\n",
            "-----------Crawling page 171-----------\n",
            "Crawl page 171 successfully , time completed : 70.48885250091553 s\n",
            "-----------Crawling page 172-----------\n",
            "Crawl page 172 successfully , time completed : 71.01936149597168 s\n",
            "-----------Crawling page 173-----------\n",
            "Crawl page 173 successfully , time completed : 68.5454158782959 s\n",
            "-----------Crawling page 174-----------\n",
            "Crawl page 174 successfully , time completed : 67.85781598091125 s\n",
            "-----------Crawling page 175-----------\n",
            "Crawl page 175 successfully , time completed : 71.62757277488708 s\n",
            "-----------Crawling page 176-----------\n",
            "Crawl page 176 successfully , time completed : 76.929514169693 s\n",
            "-----------Crawling page 177-----------\n",
            "Crawl page 177 successfully , time completed : 76.48993611335754 s\n",
            "-----------Crawling page 178-----------\n",
            "Crawl page 178 successfully , time completed : 73.82042574882507 s\n",
            "-----------Crawling page 179-----------\n",
            "Crawl page 179 successfully , time completed : 74.66147947311401 s\n",
            "-----------Crawling page 180-----------\n",
            "Crawl page 180 successfully , time completed : 69.96163654327393 s\n",
            "-----------Crawling page 181-----------\n",
            "Crawl page 181 successfully , time completed : 73.97038674354553 s\n",
            "-----------Crawling page 182-----------\n",
            "Crawl page 182 successfully , time completed : 70.78444695472717 s\n",
            "-----------Crawling page 183-----------\n",
            "Crawl page 183 successfully , time completed : 72.54016017913818 s\n",
            "-----------Crawling page 184-----------\n",
            "Crawl page 184 successfully , time completed : 74.12732481956482 s\n",
            "-----------Crawling page 185-----------\n",
            "Crawl page 185 successfully , time completed : 68.35067462921143 s\n",
            "-----------Crawling page 186-----------\n",
            "Crawl page 186 successfully , time completed : 72.32347130775452 s\n",
            "-----------Crawling page 187-----------\n",
            "Crawl page 187 successfully , time completed : 67.64365029335022 s\n",
            "-----------Crawling page 188-----------\n",
            "Crawl page 188 successfully , time completed : 71.3522253036499 s\n",
            "-----------Crawling page 189-----------\n",
            "Crawl page 189 successfully , time completed : 69.96572494506836 s\n",
            "-----------Crawling page 190-----------\n",
            "Crawl page 190 successfully , time completed : 73.70736694335938 s\n",
            "-----------Crawling page 191-----------\n",
            "Crawl page 191 successfully , time completed : 72.61955833435059 s\n",
            "-----------Crawling page 192-----------\n",
            "Crawl page 192 successfully , time completed : 72.51352047920227 s\n",
            "-----------Crawling page 193-----------\n",
            "Crawl page 193 successfully , time completed : 71.70654439926147 s\n",
            "-----------Crawling page 194-----------\n",
            "Crawl page 194 successfully , time completed : 68.30451774597168 s\n",
            "-----------Crawling page 195-----------\n",
            "Crawl page 195 successfully , time completed : 67.99577498435974 s\n",
            "-----------Crawling page 196-----------\n",
            "Crawl page 196 successfully , time completed : 67.58639430999756 s\n",
            "-----------Crawling page 197-----------\n",
            "Crawl page 197 successfully , time completed : 68.8578155040741 s\n",
            "-----------Crawling page 198-----------\n",
            "Crawl page 198 successfully , time completed : 70.3705587387085 s\n",
            "-----------Crawling page 199-----------\n",
            "Crawl page 199 successfully , time completed : 69.3269829750061 s\n",
            "-----------Crawling page 200-----------\n",
            "Crawl page 200 successfully , time completed : 67.71987771987915 s\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "data=[]\n",
        "getItems(151,200,data)\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKDtfkWuesKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7540e3-49d7-45d0-c8af-925a550f0b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON data has been converted to CSV and saved as 'output.csv'\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open('Data_151_200.json', \"w\",encoding=\"utf-8\") as json_file:\n",
        "    # Ghi thông tin vào tập tin JSON\n",
        "    json.dump(data, json_file,ensure_ascii=False, indent=4)\n",
        "df = pd.json_normalize(data)\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv('Data_151_250.csv', index=False)\n",
        "\n",
        "print(\"JSON data has been converted to CSV and saved as 'output.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chúng em chia ra cho mỗi người chay trên từng máy tính khác nhau rồi sau đó gọp dữ liệu lại"
      ],
      "metadata": {
        "id": "jLGbMvlA2ksF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Zo5jYgr2uGR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}